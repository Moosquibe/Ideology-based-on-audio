{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom made LSTM model\n",
    "Requires Pytorch 0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKDIR = \"/data/WorkData/ideology_from_audio/RESULTS/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Adding ideologies to the raw waveform database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_name(name):\n",
    "    parts = name.split(\" \")\n",
    "    if parts[-1].lower() == 'jr' or parts[-1].lower() == 'iii' or parts[-1].lower() == 'ii':\n",
    "        if len(parts) == 1:\n",
    "            return \"noname\"\n",
    "        return parts[-2].lower()\n",
    "    else:\n",
    "        return parts[-1].lower()\n",
    "\n",
    "def get_ideology(last_name):\n",
    "    group = ideologies[ideologies['last'] == last_name]\n",
    "    return group.ideology.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideologies = pd.read_csv(WORKDIR + \"ideologyfinal.txt\")\n",
    "ideologies = ideologies.drop(['Unnamed: 0', 'first'], axis = 1)\n",
    "ideologies = ideologies[ideologies.ideology != 'undefined']\n",
    "ideologies = ideologies.reset_index().drop('index', axis = 1)\n",
    "ideologies['ideology'] = ideologies['ideology'].apply(lambda x: int(float(x) > 0.5))\n",
    "ideologies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I lost the code, but here, ideology is matched to names and the hole\n",
    "# dataframe is shuffled.\n",
    "records = pd.read_csv(WORKDIR + \"final_raw_wave_ideology.csv\").drop('Unnamed: 0', axis = 1)\n",
    "records.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records.to_csv(WORKDIR + 'final_raw_wave_ideology.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting the records by words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_words = list(map(lambda s: s.upper(), ['Justice', 'Honor', 'Federal', 'Congress', \n",
    "              'Government', 'Evidence', 'Argument', 'Issue', 'Science', 'Taxation']))\n",
    "records_by_word = {}\n",
    "\n",
    "for used_word in used_words:\n",
    "    records_by_word[used_word] = records[records.Word == used_word]\n",
    "    records_by_word[used_word] = records_by_word[used_word].reset_index().drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not training on SCIENCE and TAXATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_used_words = used_words[:-2]\n",
    "train_used_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a word by word splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ratio = 0.1\n",
    "\n",
    "records_by_word_train = {}\n",
    "records_by_word_test = {}\n",
    "\n",
    "for used_word in used_words:\n",
    "    recs = records_by_word[used_word]\n",
    "    if used_word in train_used_words:\n",
    "        num_occur = len(recs.index)\n",
    "        split_index = int(num_occur * (1-valid_ratio))\n",
    "        records_by_word_train[used_word] = recs.iloc[:split_index].reset_index().drop('index', axis = 1)\n",
    "        records_by_word_test[used_word] = recs.iloc[split_index:].reset_index().drop('index', axis = 1)\n",
    "        \n",
    "    else:\n",
    "        records_by_word_test[used_word] = recs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also get a global training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_train = pd.concat(records_by_word_train.values())\n",
    "records_train = records_train.iloc[np.random.permutation(len(records_train))]\n",
    "records_train = records_train.reset_index().drop('index', axis = 1)\n",
    "records_test = pd.concat(records_by_word_test.values())\n",
    "records_test = records_test.reset_index().drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataloading scripts and auxiliary methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vstack_with_padding(a,b):\n",
    "    if len(a.shape) == 1:\n",
    "        a = a.reshape(1,-1)\n",
    "    if len(b.shape) == 1:\n",
    "        b = b.reshape(1,-1)\n",
    "        \n",
    "    if a.shape[1] > b.shape[1]:\n",
    "        b = np.hstack((b, np.zeros((b.shape[0],a.shape[1]-b.shape[1]))))\n",
    "    elif b.shape[1] > a.shape[1]:\n",
    "        a = np.hstack((a, np.zeros((a.shape[0],b.shape[1]-a.shape[1]))))\n",
    "    return np.vstack((a,b))\n",
    "\n",
    "def get_minibatch(data_df, batch_size = 32, seed = 0):\n",
    "    \"\"\" \n",
    "    Returns a minibatch of size batch_size from data_dict,\n",
    "    starting at index given by seed.\n",
    "    \"\"\"\n",
    "    N = len(data_df.index)\n",
    "    assert seed < N, \"seed out of bounds\"\n",
    "    X_batch = None\n",
    "    y_batch = []\n",
    "    for i in range(seed, min(seed + batch_size, N)):\n",
    "        row = data_df.iloc[i]\n",
    "        used_word = row.Word\n",
    "        filename = row.Filename\n",
    "        path = WORKDIR + \"/WordAudio/\" + used_word + \"/\" + filename\n",
    "        waveform, sample_rate = librosa.load(path)\n",
    "        if X_batch is None:\n",
    "            X_batch = waveform\n",
    "        else:\n",
    "            X_batch = vstack_with_padding(X_batch, waveform)\n",
    "        y_batch.append(row.Ideology)\n",
    "    return X_batch, np.array(y_batch)\n",
    "\n",
    "def get_accuracy(data_df, classifier, threshold = 0.5, verbose = True, gpu = False):\n",
    "    \"\"\"\n",
    "    Computes accuracy of classifier over examples in data_df\n",
    "    \"\"\"\n",
    "    if gpu and torch.cuda.is_available():\n",
    "        classifier = classifier.cuda()\n",
    "    correct = 0\n",
    "    N = len(data_df.index)\n",
    "    for i in range(N):\n",
    "        if verbose and i % 10 == 0:\n",
    "            print(\" {}/{}\".format(i,N))\n",
    "        row = data_df.iloc[i]\n",
    "        used_word = row.Word\n",
    "        filename = row.Filename\n",
    "        path = WORKDIR + \"/WordAudio/\" + used_word + \"/\" + filename\n",
    "        waveform, sample_rate = librosa.load(path)\n",
    "        inp = torch.Tensor(waveform).view(1,1,-1)\n",
    "        if gpu and torch.cuda.is_available():\n",
    "            inp = inp.cuda()\n",
    "        scores = classifier(inp)[0]\n",
    "        if scores[0] >= threshold:\n",
    "            y_pred = 1\n",
    "        else:\n",
    "            y_pred = 0\n",
    "        y_gt = row.Ideology\n",
    "        if y_pred == y_gt:\n",
    "            correct += 1\n",
    "            \n",
    "    return float(correct) / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training a custom classifier over the full training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conv_lstm_classifier2 import ConvLSTM\n",
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on the full training set\n",
    "classifier = ConvLSTM(\n",
    "                    conv_kernel_size = 5,\n",
    "                    conv_stride = 3,\n",
    "                    num_features = 32,\n",
    "                    pooling_kernel = 1,\n",
    "                    hidden_size = 512,\n",
    "                    num_layers = 2,\n",
    "                    num_of_classes = 2,\n",
    "                    bias = False,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(classifier, data, valid_data, num_epochs = 10, \n",
    "          batch_size = 32, verbose = False, gpu = False, print_accuracy = False):\n",
    "    if print_accuracy:\n",
    "        print(\" Computing initial accuracy on validation set...\")\n",
    "        acc = get_accuracy(valid_data, classifier.cpu(), verbose = False, gpu = gpu)\n",
    "        print(\" Initial accuracy: {}\".format(acc))\n",
    "\n",
    "    if gpu and torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        classifier = classifier.cuda()\n",
    "    else:\n",
    "        classifier = classifier.cpu()  \n",
    "        \n",
    "    optimizer = torch.optim.Adam(\n",
    "                            classifier.parameters(),\n",
    "                            betas = (0.9, 0.999),\n",
    "                            eps = 1e-08,\n",
    "                            weight_decay = 0.01)\n",
    "    \n",
    "    N = len(data.index)\n",
    "    batches_per_epoch = int(N / batch_size) + 1\n",
    "    best_acc = 0\n",
    "    best_params = classifier.state_dict()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch: {}/{}\".format(epoch + 1, num_epochs))\n",
    "        seed = 0\n",
    "        for batch in range(batches_per_epoch):\n",
    "            if batch % 50 == 0:\n",
    "                print(\"  Batch: {}/{}\".format(batch + 1, batches_per_epoch))\n",
    "            # Prepare the data\n",
    "            X_batch, y_batch = get_minibatch(data, batch_size = batch_size, \n",
    "                                             seed = seed)\n",
    "            current_batch_size = X_batch.shape[0]\n",
    "            X_batch = torch.tensor(X_batch, requires_grad = False,  dtype = torch.float)\n",
    "            y_batch = torch.tensor(y_batch, requires_grad = False)\n",
    "            \n",
    "            \n",
    "            if gpu and torch.cuda.is_available():\n",
    "                X_batch = X_batch.cuda()\n",
    "                y_batch = y_batch.cuda()\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Making an optimization step\n",
    "            optimizer.zero_grad()\n",
    "            output = classifier(X_batch.view(current_batch_size, 1, -1))\n",
    "            loss = F.cross_entropy(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # End of batch stuff\n",
    "        seed += batch_size\n",
    "        if print_accuracy:\n",
    "            print(\" Computing accuracy on given test set...\")\n",
    "            acc = get_accuracy(valid_data, classifier, verbose = False, gpu = gpu)\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_params = classifier.state_dict()\n",
    "            print(\" Accuracy: {}\".format(acc))\n",
    "    classifier.load_state_dict(best_params)\n",
    "    return classifier.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.set_device(2)\n",
    "valid_data = pd.concat([records_by_word_test[\"SCIENCE\"],records_by_word_test[\"TAXATION\"]])\n",
    "classifier = train(classifier, records_train, valid_data, num_epochs = 10, batch_size = 16, gpu = True, print_accuracy=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
