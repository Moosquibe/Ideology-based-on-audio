{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data gathering scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Gathering the phonetics data\n",
    "\n",
    "The data set was initially as follows: \n",
    "\n",
    "For a given case (e.g. 2006_04_1350), we have:\n",
    "-- A text-grid file `2006_04_1350.TextGrid`, found in `/data/Dropbox/Data/Supreme_Court_Audio/Oyez_vowels/FAVE/oyez_full/2006`\n",
    "This is the breakdown by sentences/paragraphs. It contains a number N of « items » which is indicated at the beginning as the size parameter (here, size = 11). Each item corresponds to one person speaking.\n",
    "-- Corresponding files in `/data/Dropbox/Data/Supreme_Court_Audio/Oyez_vowels/FAVE/FAVE-extract/`\n",
    "For any given hearing, there are 4 * M files, where M is less than or equal to N - the number of items (or size) as above. For 2006_04_1350 for example, N is 11 but M is 4. File K corresponds to speaker K. The relevant files for us are those named \n",
    "`2006_04_1350_sK_norm.txt`\n",
    "where K is the index of the speaker. They contained the breakdown of speaker K speech by word/vowels, together with the relevant phonetics information for each vowel (the duration, the two first formants, and more formant data)\n",
    "\n",
    "From the first file (the `.TextGrid`), we find the name of speaker K, and we then gather the phonetics data associated to this speaker by going through the corresponding `_norm.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "## Gathers the data from formants, using the names in the TextGrids\n",
    "####\n",
    "# encoding: utf-8\n",
    "import os\n",
    "import io\n",
    "\n",
    "TEXTGRIDS_FOLDERS = \"/data/Dropbox/Data/Supreme_Court_Audio/Oyez_vowels/FAVE/oyez_full/\"\n",
    "FORMANTS_FOLDERS = \"/data/Dropbox/Data/Supreme_Court_Audio/Oyez_vowels/FAVE/FAVE-extract/\"\n",
    "RESULT_FOLDERS = \"\"\n",
    "\n",
    "#######\n",
    "### Takes a file with formants (the norm.txt ones)\n",
    "### extracts the words with more than 1 syllable\n",
    "### and adds (in 'ouput') a data entry of the type\n",
    "### word, speaker, [list of the syllables and the formants data for each syllable]\n",
    "#####\n",
    "def treat_file(filetotreat, output, speaker):\n",
    "    lines = filetotreat.readlines()\n",
    "    current_word = \"--\"\n",
    "    word_occurrence = []\n",
    "\n",
    "    for line in (l.split() for l in lines[3:] if len(l.split()) == 26): ## Skip the first 3 lines, and avoid weird entries\n",
    "        collapsed_line = collapse_line(line)\n",
    "        if (collapsed_line[0] == current_word or current_word == \"--\"):\n",
    "            word_occurrence.append(collapsed_line)\n",
    "        else:\n",
    "            if (len(word_occurrence) > 1): ## We only keep words with more than one syllable\n",
    "                add_occurrence(current_word, word_occurrence, speaker, output)\n",
    "            word_occurrence = [collapsed_line]\n",
    "        current_word = collapsed_line[0]\n",
    "\n",
    "    ### Last word of the file\n",
    "    if (len(word_occurrence) > 1):\n",
    "        add_occurrence(current_word, word_occurrence, speaker, output)\n",
    "\n",
    "    return True\n",
    "\n",
    "def add_occurrence(word, occurrence, speaker, output):\n",
    "    output.write(word+','+speaker+','+str(occurrence)+'\\n')\n",
    "\n",
    "def collapse_line(line):\n",
    "    vowel = line[0]\n",
    "    # stress = line[1]\n",
    "    word = line[2]\n",
    "    formant_first = line[3]\n",
    "    formant_second = line[4]\n",
    "    # t 5, beg 6, end 7\n",
    "    duration = line[8]\n",
    "    # cd 9, fm 10, fp 11, fv 12, ps 13, fs 14, style 15, glide 16\n",
    "    formant_first_20 = line[17]\n",
    "    formant_second_20 = line[18]\n",
    "    formant_first_35 = line[19]\n",
    "    formant_second_35 = line[20]\n",
    "    formant_first_50 = line[21]\n",
    "    formant_second_50 = line[22]\n",
    "    formant_first_65 = line[23]\n",
    "    formant_second_65 = line[24]\n",
    "    ff_summary = [formant_first_20, formant_first_35, formant_first_50, formant_first_65]\n",
    "    fs_summary = [formant_second_20, formant_second_35, formant_second_50, formant_second_65]\n",
    "    line_collapsed = [word, vowel, formant_first, formant_second, duration, ff_summary, fs_summary]\n",
    "    return line_collapsed\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.chdir(RESULT_FOLDERS)\n",
    "    output = open(\"wordssyllablesformants.txt\", \"a\")\n",
    "    os.chdir(TEXTGRIDS_FOLDERS)\n",
    "    years = os.listdir() ### List all the years\n",
    "    for year in (y for y in years if y.isnumeric()):\n",
    "        os.chdir(TEXTGRIDS_FOLDERS+year)\n",
    "        list_hearings = os.listdir() ### List all the hearings for a given year\n",
    "        for hearing in (h for h in list_hearings if h.endswith('.TextGrid')):\n",
    "            hearing_textgrid = open(TEXTGRIDS_FOLDERS+year+'/'+hearing, 'r', encoding='utf-8')\n",
    "            hearing_textgrid_lines = hearing_textgrid.readlines()\n",
    "            item = 0\n",
    "            for line in (l for l in hearing_textgrid_lines if l.strip().startswith(\"name =\")): ### Looking for the speaker name\n",
    "                item += 1\n",
    "                speaker = (line.strip())[8:-1] ## Only keep the name\n",
    "                if (item < 10):\n",
    "                    name_file = hearing.strip('.TextGrid')+'_s0'+str(item)+'_norm.txt'\n",
    "                else:\n",
    "                    name_file= hearing.strip('.TextGrid')+'_s'+str(item)+'_norm.txt'\n",
    "                try:\n",
    "                    os.chdir(FORMANTS_FOLDERS+year+'_vowels') ### Opening the formant file\n",
    "                    filetotreat = open(name_file)\n",
    "                except FileNotFoundError:\n",
    "                    break\n",
    "                # os.chdir(RESULT_FOLDERS+year)\n",
    "                # output = open(\"result_\"+name_file, 'a')\n",
    "                treat_file(filetotreat, output, speaker) #### Treating the formant file\n",
    "                filetotreat.close()\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Listing words and names\n",
    "\n",
    "In an exploratory phase, we figure out the words most pronounced by our speakers, as well as a list of all the speakers that appear.\n",
    "\n",
    "The number of speakers is of order 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the list of the words that are pronounced in the hearing\n",
    "# Keep only the one that are pronounced more than 100 times\n",
    "input_file = \"/data/WorkData/ideology_from_audio/RESULTS/wordssyllablesformants.txt\"\n",
    "output_file = \"/data/WorkData/ideology_from_audio/RESULTS/listwords.txt\"\n",
    "\n",
    "listofwords = open(input_file, \"r\")\n",
    "orderedwords = open(output_file, \"a\")\n",
    "lines = listofwords.readlines()\n",
    "words = {}\n",
    "\n",
    "for line in lines:\n",
    "    word = line.split(',')[0]\n",
    "    if (word in words):\n",
    "        words[word] += 1\n",
    "    else:\n",
    "        words[word] = 1\n",
    "\n",
    "listofwords.close()\n",
    "\n",
    "words_rev = [(words[word], word) for word in words]\n",
    "words_rev.sort(reverse=True)\n",
    "\n",
    "for value, word in words_rev:\n",
    "    if (value > 100):\n",
    "        orderedwords.write(word+\" \"+str(value)+\"\\n\")\n",
    "    else:\n",
    "        break\n",
    "orderedwords.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "# Write down the list of people who say something in the hearing dataset\n",
    "\n",
    "input_file = \"/data/WorkData/ideology_from_audio/RESULTS/wordssyllablesformants.txt\"\n",
    "output_file = \"/data/WorkData/ideology_from_audio/RESULTS/listpeople.txt\"\n",
    "\n",
    "listofwords = open(input_file, \"r\")\n",
    "peoples = open(output_file, \"a\")\n",
    "lines = listofwords.readlines()\n",
    "\n",
    "names = []\n",
    "for line in lines:\n",
    "    name = line.split(',')[1]\n",
    "    if not (name in names):\n",
    "        names.append(name)\n",
    "names.sort()\n",
    "\n",
    "for name in names:\n",
    "    peoples.write(name+\"\\n\")\n",
    "    \n",
    "listofwords.close()\n",
    "peoples.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Searching one's ideology\n",
    "\n",
    "We now have to connect speakers with their suspected ideology (Democrat or Republican). As suggested by our advisors, we use the DIME (Database on Ideology, Money in politics and Elections) database, developed by Adam Bonica and available at https://data.stanford.edu/dime. This dataset lists the financial contributions to political campaigns in the USA. Some basic difficulties appear:\n",
    "- The database is very large, with several dozens of millions of entries, and a naive thorough search takes too much time if it has to be done for ~1000 speakers.\n",
    "- Some common names may appear several times.\n",
    "- How to identify a speaker in the donation list?\n",
    "We also encounter the less obvious issue:\n",
    "- It is fairly common that a given person will give money to both parties!\n",
    "Inspired by the previous group, we chose the following approach:\n",
    "1. We go through the list of donation exactly once, and only keep the entry if the \"job/activity\" contains a word relevant for us: lawyer, justice, attorney... \n",
    "This effectively reduces the size of the list by a factor 10--100.\n",
    "2. Then for each speaker, we go through the reduced list and try to match first/last name with the first/last name entry of the donation. **Here we make the following bet: there is only one person with a given first&last name who happens to work as a lawyer. A more careful procedure would involve checking the middle name (not always present in both tables) or the adresses and listing the possible collisions.**\n",
    "3. If a speaker is not present, his/her ideology is set to \"Undefined\". Otherwise, we take the average amount\n",
    "$$\n",
    "\\textbf{Ideology}(\\textrm{speaker}) = \\frac{\\textrm{Donation of the given speaker for Republican candidates}}{\\textrm{Donation of the given speaker}}\n",
    "$$\n",
    "Hence an ideology of $0$ corresponds to a pure Democrat and ideology of $1$ corresponds to pure Republican.\n",
    "\n",
    "We run this on two datasets: the 2008 political campaign (including State and Local elections) and the combined Presidential campaigns (all Presidential elections since the mid-70's) and we take the average (which is admittedly not a proper barycenter).\n",
    "\n",
    "This way, we obtain an ideology label for ~500 speakers over ~900."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "# Extract the lawyers from the donation dataset, to avoid looping on a huge file\n",
    "import csv\n",
    "\n",
    "DONATIONS_CSV = \"contribDB_president.csv\"\n",
    "DONATIONS_LAWYERS = \"contriblawyers_from_presidential.csv\"\n",
    "\n",
    "file_donations=open(DONATIONS_CSV, 'r')\n",
    "file_contributions=open(DONATIONS_LAWYERS, 'w')\n",
    "\n",
    "def is_lawyer(job):\n",
    "    return (\"judge\" in job or \"lawyer\" in job or \"attorney\" in job or \"advocate\" in job or \"law\" in job or \"justice\" in job)\n",
    "\n",
    "contributions = csv.writer(file_contributions)\n",
    "\n",
    "for donation in csv.reader(file_donations):\n",
    "    if is_lawyer(donation[19]):\n",
    "        contributions.writerow(donation)\n",
    "        \n",
    "file_donations.close()\n",
    "file_contributions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "# Find ideology from a donation dataset\n",
    "import csv\n",
    "\n",
    "DONATIONS_CSV = \"contriblawyers_from_presidential.csv\"\n",
    "PEOPLE_FILE = \"listpeople.txt\"\n",
    "LOG_FILE = \"results_ideologt_from_presidential.txt\"\n",
    "\n",
    "log = open(LOG_FILE, \"w\")\n",
    "\n",
    "#### Form the list of speakers\n",
    "file_people = open(PEOPLE_FILE, 'r')\n",
    "list_people = file_people.readlines()\n",
    "\n",
    "speakers = []\n",
    "\n",
    "for people in list_people:\n",
    "    name_components = people.split()\n",
    "    if len(name_components) == 2:\n",
    "        first_name = name_components[0].lower()\n",
    "        last_name = name_components[1].lower()\n",
    "    elif len(name_components) > 2:\n",
    "        first_name = name_components[0].lower()\n",
    "        last_name = name_components[2].lower()\n",
    "    speakers.append({'FN':first_name, 'LN':last_name})\n",
    "\n",
    "file_people.close()\n",
    "\n",
    "def is_lawyer(job):\n",
    "    return (\"judge\" in job or \"lawyer\" in job or \"attorney\" in job or \"advocate\" in job or \"law\" in job or \"justice\" in job)\n",
    "\n",
    "def parcours_donation(speaker):\n",
    "    sum_dem = 0\n",
    "    sum_rep = 0\n",
    "    file_donations = open(DONATIONS_CSV, 'r')\n",
    "    for donation in csv.reader(file_donations):\n",
    "        amount = float(donation[3])\n",
    "        first_name = donation[8]\n",
    "        last_name = donation[7]\n",
    "        party = donation[24]\n",
    "        job = donation[19]\n",
    "        if first_name == speaker['FN'] and last_name == speaker['LN'] and is_lawyer(job):\n",
    "            print(\"FOUND ONE!\")\n",
    "            print(\"An amount of \"+str(amount))\n",
    "            if party==\"100\":\n",
    "                sum_dem += amount\n",
    "                print(\"A democrat!\")\n",
    "            elif party==\"200\":\n",
    "                sum_rep += amount\n",
    "                print(\"A republican !\")\n",
    "    file_donations.close()\n",
    "    print(\"End of run!\")\n",
    "    if (sum_dem+sum_rep == 0):\n",
    "        return -1\n",
    "    else:\n",
    "        return (sum_rep)/(sum_dem+sum_rep)\n",
    "\n",
    "#### This is a silly \"human\" way of writing down the results and we shall pay a small price later on\n",
    "for speaker in speakers:\n",
    "    ideology = parcours_donation(speaker)\n",
    "    if ideology == -1:\n",
    "        log.write(speaker['FN']+' '+speaker['LN']+' is undefined \\n')\n",
    "    else:\n",
    "        log.write(speaker['FN']+' '+speaker['LN']+' is '+str(ideology)+' \\n')\n",
    "\n",
    "log.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
